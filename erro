import json

if rag_response and isinstance(rag_response, dict) and "error" not in rag_response:
    try:
        # Parse the JSON string inside the response (if it's stringified)
        if isinstance(rag_response.get("response"), str):
            inner_response = json.loads(rag_response["response"])
        else:
            inner_response = rag_response["response"]

        rag_answer = inner_response.get("message", "").strip()
        print(f"rag_answer::: {rag_answer}")

        if rag_answer:
            # 2. Ask Gemini to rate how well the RAG answer solves the problem
            eval_prompt = f"""
            You are an evaluator. A troubleshooting assistant has generated a response to the user's cloud issue.

            ---
            **Final Problem Statement:**
            {final_problem_statement}

            **RAG Answer:**
            {rag_answer}

            ---
            On a scale of 0 to 100, how well does the RAG answer directly solve or address the final problem statement?

            Only respond with a number between 0 and 100.
            """
            try:
                score_response = llm.invoke([HumanMessage(content=eval_prompt)])
                score_str = score_response.content.strip()
                print(f"score_str::: {score_str}")
                score = int(''.join(filter(str.isdigit, score_str)))
            except Exception as eval_err:
                logger.warning("RAG evaluation by LLM failed: %s", eval_err)
                score = 0

            print(f"score::: {score}")
            if score >= 70:
                documentation = rag_answer.split("\n")
                source = "rag"
    except Exception as parse_err:
        logger.error("Failed to parse RAG response: %s", parse_err)
