import json
import sys
import operator
from pathlib import Path
from typing import List, Literal, TypedDict, Annotated

from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import JsonOutputParser

# Local imports
sys.path.insert(0, str(Path(__file__).parent.absolute()))
from utils.config import llm, logger


# -------------------------------
# State Schema
# -------------------------------
class InputIntentAgentState(TypedDict):
    dialog: Annotated[List[str], operator.add]
    last_input: str
    status: Literal["complete", "incomplete", "out_of_scope", "error"]
    final_problem_statement: str
    messages: Annotated[List[dict], operator.add]
    flow_type: str
    documentation: List[str]
    next_question: str
    thread_id: str
    source: str


# -------------------------------
# Input Agent Definition
# -------------------------------
def run_input_intent_agent(state: dict) -> dict:
    parser = JsonOutputParser()
    logger.info("State received by InputAgent:", state)  # üîç Debug

    dialog = state.get("dialog", [])
    last_input = state.get("last_input", "")

    # Keep last 10 dialog turns only
    if len(dialog) > 10:
        dialog = dialog[-10:]

    # Prompt
    prompt = f"""
**Initiate Interaction:** If this is the very first interaction (i.e., 'Dialog so far' is empty), begin with a warm and open-ended greeting, inviting the user to share their problem. For example: "Hi there! I'm here to help with any cloud or IT issues you're facing. What problem are you encountering today?"

You are a helpful and intelligent assistant. Your primary role is to act as the initial point of contact, diligently collecting all essential details about a user's technical issue. Your goal is to construct a clear, comprehensive, and complete problem statement by asking intelligent, context-aware follow-up questions.

**You must adhere to the following:**

1. **Initial Intent Classification:**
   * `problem_statement`: The user is reporting an active issue they are facing.
   * `general_query`: The user is asking for steps to perform a task, seeking general information, or asking for a guide.

2. **Information Collection Mandate (Conditional Questioning):**

   For `problem_statement` issues, gather:
   * Cloud Provider (AWS, Azure, GCP, OCI)
   * Affected Service
   * Error Message / Symptom
   * Resource ID(s)
   * Region / AZ
   * Action Attempted
   * Contextual Details (worked before? changes?)
   * Project name (if GCP)
   * Preferred Output Method (CLI, Console, API, All)
   * Environment (prod/non-prod)
   * Vsad

   For `general_query` issues, gather:
   * Cloud Provider
   * Affected Service
   * Related Services
   * Action Requested
   * Resource ID(s)
   * Preferred Output Method
   * Project name (if applicable)
   * Environment
   * Vsad

3. **Intelligent Questioning (if status=incomplete):**
   * Ask for only missing details.
   * Be polite and empathetic.
   * Never ask for info already present.
   * For `general_query`, don‚Äôt ask about errors.

4. **Strict Output Format:**

   For Cloud/IT Issues:
   {{
     "dialog": [... list of user messages ...],
     "final_problem_statement": "... summary of issue including all collected details...",
     "status": "complete" | "incomplete",
     "next_question": "If status is incomplete, ask this question next"
   }}

   For Out-of-Scope Issues:
   {{
     "dialog": [... list of user messages ...],
     "final_problem_statement": "N/A",
     "status": "out_of_scope",
     "next_question": "I'm sorry, but my purpose is to help with cloud and IT issues. Please let me know about a specific problem you are facing."
   }}

   * `final_problem_statement` must be a concise concatenation of all collected key-value pairs (even if 'N/A').

**Context for this interaction:**
Dialog so far:
{dialog}
New input from user:
{last_input}
    """

    try:
        # Call model
        response = llm.invoke([HumanMessage(content=prompt)])
        logger.info(f"üîé LLM raw output: {response}")

        # Parse JSON with enforced schema
        parsed = parser.invoke(response.content)

        if not parsed:
            raise ValueError("Invalid JSON response from LLM")

        # Append messages to dialog
        dialog.append({"role": "user", "content": last_input})
        if parsed.get("status") == "incomplete":
            dialog.append({"role": "assistant", "content": parsed.get("next_question", "")})

        return {
            "dialog": dialog,
            "final_problem_statement": parsed.get("final_problem_statement", ""),
            "status": parsed.get("status", "incomplete"),
            "next_question": parsed.get("next_question", ""),
            "last_input": ""
        }

    except Exception as e:
        logger.error(f"‚ö†Ô∏è Failed to parse LLM response: {str(e)}")
        dialog.append({"role": "user", "content": last_input})
        return {
            "dialog": dialog,
            "status": "error",
            "message": f"LLM returned invalid output. Error: {str(e)}",
            "final_problem_statement": "",
            "next_question": "",
            "last_input": ""
        }
