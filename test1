# First, install the necessary libraries.
# If you plan to deploy, make sure you have the [agent_engines] extra.
# For local-only testing, [langchain] and langchain_google_vertexai are sufficient.
# pip install --upgrade --quiet google-cloud-aiplatform[agent_engines,langchain] langchain_google_vertexai langgraph cloudpickle httpx pydantic

import vertexai
from vertexai import agent_engines # This is needed for deployment
import os
from typing import Literal
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_google_vertexai import ChatVertexAI
from langgraph.graph import END, MessageGraph
from langgraph.prebuilt import ToolNode

# --- Configuration for Vertex AI ---
# IMPORTANT: Replace these with your actual Google Cloud Project ID and a GCS bucket.
# The staging bucket is required for agent_engines.create()
PROJECT_ID = "[YOUR_PROJECT_ID]"
LOCATION = "us-central1" # Or your desired region
STAGING_BUCKET = "gs://[YOUR_STAGING_BUCKET]" # A GCS bucket for staging

# Initialize Vertex AI for model interaction (local or deployed)
# The staging_bucket is only needed if you plan to use agent_engines.create()
vertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)


# 1. Define your tools
def get_product_details(product_name: str):
    """Retrieves details for a given product."""
    print(f"DEBUG: Calling get_product_details for: {product_name}") # Added for visibility
    if product_name.lower() == "laptop":
        return {"name": "Laptop Pro", "price": 1200, "description": "Powerful laptop for professionals."}
    else:
        return {"name": product_name, "price": "N/A", "description": "Details not available."}

# 2. Create your LangGraph app
class SimpleLangGraphApp:
    def __init__(self, project: str, location: str) -> None:
        self.project_id = project
        self.location = location
        self.graph = None # Initialize graph to None

    def set_up(self) -> None:
        # For local execution and deployment, ensure your Google Cloud authentication is set up
        # (e.g., via `gcloud auth application-default login`)
        model = ChatVertexAI(model="gemini-2.0-flash")
        builder = MessageGraph()
        model_with_tools = model.bind_tools([get_product_details])

        # Define your graph logic
        builder.add_node("agent", model_with_tools)
        tool_node = ToolNode([get_product_details]) # Pass the tool directly to ToolNode
        builder.add_node("call_tool", tool_node)

        # Define conditional edges: Agent decides whether to call a tool or finish
        def should_continue(state: list[BaseMessage]):
            last_message = state[-1]
            # If the last message contains tool calls, we execute the tools
            if last_message.tool_calls:
                return "continue_tool"
            # Otherwise, the agent is done and we can end the graph
            return "end"

        builder.add_conditional_edges(
            "agent",
            should_continue,
            {
                "continue_tool": "call_tool",
                "end": END
            }
        )
        builder.add_edge("call_tool", "agent") # Tool result goes back to agent

        builder.set_entry_point("agent")
        self.graph = builder.compile()

    def query(self, input_message: str):
        # This is how your deployed agent will be queried, and also how you query locally
        if self.graph is None:
            raise RuntimeError("Graph not set up. Call set_up() first.")
        # LangGraph invoke expects a dictionary with 'messages' key
        print(f"DEBUG: Invoking graph with message: '{input_message}'") # Added for visibility
        result = self.graph.invoke({"messages": [HumanMessage(content=input_message)]})
        return result

def run_local_app():
    """
    Function to run the LangGraph application locally.
    """
    print("\n--- Running LangGraph App Locally ---")
    local_app = SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION)
    local_app.set_up() # Compile the graph

    # Test Queries
    response1 = local_app.query("What are the details for a laptop?")
    print("\nLocal Response 1: 'What are the details for a laptop?'")
    print(response1)

    response2 = local_app.query("Tell me a joke.")
    print("\nLocal Response 2: 'Tell me a joke.' (No tool call expected)")
    print(response2)

    response3 = local_app.query("What about a bicycle?")
    print("\nLocal Response 3: 'What about a bicycle?' (Unknown product)")
    print(response3)

def deploy_agent_to_cloud(langgraph_app_instance):
    """
    Function to deploy the LangGraph application to Vertex AI Agent Engines.
    IMPORTANT: This requires specific Google Cloud setup (permissions, project ID, staging bucket).
    """
    print("\n--- Attempting to Deploy Agent to Vertex AI Agent Engines ---")
    print(f"Using Project: {PROJECT_ID}, Location: {LOCATION}, Staging Bucket: {STAGING_BUCKET}")

    try:
        remote_agent = agent_engines.create(
            langgraph_app_instance, # Pass the initialized LangGraph app instance
            requirements=[
                "google-cloud-aiplatform[agent_engines,langchain]",
                "cloudpickle==3.0.0",
                "pydantic>=2.10",
                "httpx",
                "langgraph",
                # Add any other libraries your tools or graph might need
                # Example if you had pandas: "pandas"
            ],
            display_name="My LangGraph Agent (Deployed)",
            description="A sample agent deployed from a LangGraph application."
        )
        print(f"\nSuccessfully deployed agent! Resource name: {remote_agent.resource_name}")
        print("You can now query the deployed agent (this will incur cloud costs):")

        # Query the deployed agent
        deployed_response = remote_agent.query("What are the details for a laptop?")
        print("\nDeployed Agent Response: 'What are the details for a laptop?'")
        print(deployed_response)

        deployed_response_joke = remote_agent.query("Tell me a funny story.")
        print("\nDeployed Agent Response: 'Tell me a funny story.'")
        print(deployed_response_joke)


    except Exception as e:
        print(f"\nERROR: Failed to deploy agent. Please check your configuration, permissions, and ensure the staging bucket exists and is accessible.")
        print(f"Error details: {e}")
        print("Common issues: Incorrect PROJECT_ID, STAGING_BUCKET, or missing IAM permissions (e.g., roles/aiplatform.admin, roles/storage.admin).")


def main():
    """
    Main function to orchestrate local running and optional cloud deployment.
    """
    # --- Step 1: Run Locally (Recommended for development and testing) ---
    run_local_app()

    # --- Step 2: Optional Cloud Deployment ---
    # UNCOMMENT THE FOLLOWING BLOCK ONLY IF YOU INTEND TO DEPLOY TO GOOGLE CLOUD.
    # Make sure to replace YOUR_PROJECT_ID and YOUR_STAGING_BUCKET above.
    # Also, ensure your local environment is authenticated to GCP
    # (`gcloud auth application-default login`) and has necessary IAM permissions.

    # print("\n\n--- Deployment Section (Optional) ---")
    # print("To deploy, uncomment the 'deploy_agent_to_cloud' call in main().")
    # print("Remember to replace PROJECT_ID and STAGING_BUCKET values.")
    #
    # # Instantiate the app again for deployment, as deployment expects an instance
    # app_for_deployment = SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION)
    # app_for_deployment.set_up() # Compile the graph for the instance being deployed
    #
    # deploy_agent_to_cloud(app_for_deployment)


if __name__ == "__main__":
    main()
