pip install --upgrade --quiet google-cloud-aiplatform[agent_engines,langchain]

import vertexaifrom vertexai import agent_enginesimport osfrom typing import Literalfrom langchain_core.messages import BaseMessage, HumanMessagefrom langchain_google_vertexai import ChatVertexAIfrom langgraph.graph import END, MessageGraphfrom langgraph.prebuilt import ToolNodefrom vertexai import agent_engines

PROJECT_ID = "[your-project-id]"LOCATION = "us-central1" # Or your desired regionSTAGING_BUCKET = "gs://[your-staging-bucket]" # A GCS bucket for stagingvertexai.init(project=PROJECT_ID, location=LOCATION, staging_bucket=STAGING_BUCKET)


# 1. Define your toolsdef get_product_details(product_name: str):    """Retrieves details for a given product."""    if product_name.lower() == "laptop":        return {"name": "Laptop Pro", "price": 1200, "description": "Powerful laptop for professionals."}    else:        return {"name": product_name, "price": "N/A", "description": "Details not available."}

# 2. Create your LangGraph appclass SimpleLangGraphApp:    def __init__(self, project: str, location: str) -> None:        self.project_id = project        self.location = location

    def set_up(self) -> None:        model = ChatVertexAI(model="gemini-2.0-flash")        builder = MessageGraph()        model_with_tools = model.bind_tools([get_product_details])

        builder.add_node("tools", model_with_tools)        tool_node = ToolNode([get_product_details])

        # Define your graph logic (simplified for illustration)        builder.add_node("agent", model_with_tools)        builder.add_node("call_tool", tool_node)

        builder.add_edge("agent", "call_tool") # Agent decides to call a tool        builder.add_edge("call_tool", "agent") # Tool result goes back to agent

        builder.set_entry_point("agent")        builder.set_finish_point("agent") # Agent decides when to finish

        self.graph = builder.compile()

    def query(self, input_message: str):        # This is how your deployed agent will be queried        return self.graph.invoke({"messages": [HumanMessage(content=input_message)]})

# Assuming PROJECT_ID and LOCATION are already defined from step 1# and SimpleLangGraphApp is defined as above

# Instantiate your LangGraph applanggraph_app = SimpleLangGraphApp(project=PROJECT_ID, location=LOCATION)langgraph_app.set_up() # Call set_up to compile the graph

# Deploy to Agent Engineremote_agent = agent_engines.create(    langgraph_app,    requirements=[        "google-cloud-aiplatform[agent_engines,langchain]",        "cloudpickle==3.0.0",        "pydantic>=2.10", # Adjust pydantic version if needed        "httpx",        "langgraph",        # Add any other libraries your tools or graph might need    ],    display_name="My LangGraph Agent")

print(f"Deployed agent resource name: {remote_agent.resource_name}")

response = remote_agent.query("What are the details for a laptop?")print(response)
